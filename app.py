import streamlit as st
import os
import fitz  # PyMuPDF
from openai import OpenAI
from groq import Groq
from st_copy_to_clipboard import st_copy_to_clipboard
import time
import re
import requests # New import for URL fetching

# --- Configuration & Constants ---

# API Key Retrieval
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
JINA_API_KEY = os.environ.get("JINA_API_KEY") # NEW: Jina API Key
APP_PASSWORD = os.environ.get("PASSWORD") # Password for app access

# --- IMPORTANT: These models are specially chosen for the app ---
MODEL_OPTIONS = {
    "Llama-3.3-70B (Groq)": {"id": "llama-3.3-70b-versatile", "provider": "groq"},
    "GPT-4.1 (OpenAI)": {"id": "gpt-4.1", "provider": "openai"},
    "Llama-3.1-8B-Instant (Groq)": {"id": "llama-3.1-8b-instant", "provider": "groq"}
}
DEFAULT_LLM_A_NAME = "Llama-3.3-70B (Groq)"
DEFAULT_LLM_B_NAME = "Llama-3.3-70B (Groq)"

# Task Definitions
TASK_PARAGRAPH_SUMMARY = "Paragraph Summary"
TASK_EXPLAIN_MAIN_SUBJECT = "Explain Main Subject"
TASK_THIELIAN_LENS = "Thielian Lens Insight"
TASK_IDENTIFY_MOVEMENT = "Identify Potential Movement"
TASK_LEARNING_CHUNKS = "Learning Chunks"
TASK_OPTIONS = [TASK_PARAGRAPH_SUMMARY, TASK_EXPLAIN_MAIN_SUBJECT, TASK_THIELIAN_LENS, TASK_IDENTIFY_MOVEMENT, TASK_LEARNING_CHUNKS]
DEFAULT_TASK = TASK_PARAGRAPH_SUMMARY

# --- Prompts ---

PROMPT_SUMMARY_GENERATION = """
**Your Task:** Generate a high-quality, concise, and comprehensive paragraph summary of the provided [Source Text].

**Instructions for Crafting a High-Quality Paragraph Summary:**
1.  **Understand Thoroughly:** Read the [Source Text] carefully to identify its central theme, main arguments, key findings, and essential supporting information.
2.  **Synthesize, Don't Just Extract:** Your summary should be a new piece of writing that expresses the core ideas of the source in your own words (as an LLM). Avoid simply copying and pasting sentences from the original text. The goal is true synthesis.
3.  **Accuracy is Paramount:** Ensure the summary accurately reflects the meaning and factual content of the [Source Text]. Do not introduce any information, interpretations, or opinions not explicitly present in the original.
4.  **Focus on Key Information:** Identify and include only the most important points necessary to convey the essence of the source. Omit minor details, redundant information, or tangential discussions.
5.  **Single, Coherent Paragraph:** The entire summary must be presented as a single, well-structured paragraph. It should flow logically, with clear transitions between ideas, making it easy for someone to understand the core message without having read the original text.
6.  **Conciseness:** Be as brief as possible while still covering the essential information. Every sentence should contribute to the summary's purpose.
7.  **Neutral and Objective Tone:** Unless the source text itself has a highly subjective tone that is central to its message, maintain a neutral and objective perspective.

**[Source Text]:**
\"\"\"
{source_text}
\"\"\"

**Paragraph Summary:**
"""

PROMPT_SUMMARY_VERIFICATION = """
**Your Role:** You are a meticulous Quality Assurance Editor, specializing in the evaluation of paragraph summaries.
**Your Task:**
Your goal is to assess the provided [Paragraph Summary] against the original [Source Text] to determine if it is a "good" summary according to the criteria below. Based on this assessment, you will output either a 'GREEN LIGHT' (if the summary is satisfactory) or a 'RED LIGHT' (if it has significant issues).

**Inputs:**
1.  **[Source Text]:** The full original text from which the summary was derived.
2.  **[Paragraph Summary]:** The single paragraph summary generated by another AI.

**Evaluation Criteria for a "Good Paragraph Summary":**
1.  **Accuracy:**
    * Does the summary faithfully represent the core information and meaning of the [Source Text]?
    * Does it avoid introducing any factual errors, new information not present in the source, or misrepresentations?
2.  **Key Information Coverage (Comprehensiveness for a Paragraph):**
    * Does the summary successfully capture the main idea(s) and the most crucial supporting points of the [Source Text] that are essential for understanding its essence?
    * Are any critical aspects or the central message of the source omitted, making the summary incomplete in its representation of the core content?
3.  **Conciseness and Focus:**
    * Is the summary appropriately concise and focused, avoiding unnecessary details, fluff, or redundant information that doesn't contribute to the main points?
    * Is it well-contained within a single paragraph without feeling overly dense or rushed?
4.  **Coherence and Readability:**
    * Is the paragraph well-written, with ideas flowing logically and sentences connecting smoothly?
    * Is it easy to understand for someone who has not read the original [Source Text]?
5.  **Neutrality and Objectivity:**
    * Does the summary maintain a neutral and objective tone, reflecting the information as presented in the [Source Text] (unless the source itself is clearly subjective and the summary needs to reflect that specific tone)?
6.  **Format Adherence:**
    * Is the summary correctly presented as a single, coherent paragraph?

**Output Instructions:**
* If the [Paragraph Summary] substantially meets **all** the above criteria to a good standard (minor stylistic preferences or very trivial, non-critical omissions might be acceptable, but there should be no significant flaws), respond with **only** the following words:
    `GREEN LIGHT`
* If the [Paragraph Summary] has **one or more significant flaws** in any of the evaluated criteria (especially concerning Accuracy or Key Information Coverage), respond with:
    `RED LIGHT`
    Immediately followed by a brief, bulleted list identifying the **primary reason(s)** for the 'RED LIGHT' (focus on the 1-3 most critical issues).

---
**Please begin your assessment.**
**[Source Text]:**
\"\"\"
{source_text}
\"\"\"
**[Paragraph Summary]:** \"\"\"
{step1_output}
\"\"\"
**Assessment Output:**
"""

PROMPT_EXPLAINER_TASK_FOR_LLM_A = """
You will be provided with a [Source Text]. Your first task is to identify the main subject or core concept discussed within this document.
Once you have identified this main subject/core concept, your second task is to explain it comprehensively based *only* on the information present in the [Source Text].
Use the following detailed structure and instructions for your explanation:

--- START OF EXPLAINER INSTRUCTIONS ---
You are an expert explainer. Your task is to break down the concept provided (which is the main subject of the document you've read) into clear, understandable parts for someone who is curious but not necessarily an expert.

# Instructions for the Explanation:
- Your explanation must be based on the information found within the [Source Text]. Do not introduce external knowledge unless it's for a general analogy clearly marked as such.
- Explain it step by step, avoiding jargon unless necessary and present in the [Source Text].
- If you use any technical terms from the [Source Text], define them simply based on their context in the document.
- Use analogies or metaphors where helpful to clarify concepts found in the text.
- Highlight not just *what* the main subject is, but *how* and *why* it works or is significant, according to the [Source Text].
- Structure your explanation clearly.

# Output Format for the Explanation:
**Concept:** [State the main subject/core concept you have identified from the Source Text here]

**What It Is:**
A simple definition of this concept, based on the Source Text.

**How It Works (or Key Aspects):**
A step-by-step explanation, mechanism, or key aspects of this concept, based on the Source Text.

**Why It Matters (according to the Source Text):**
Explain the significance, use cases, or implications of this concept as indicated in the Source Text.

**Analogy (Optional, if applicable and helps clarify):**
Use a metaphor or real-world comparison to help visualize the concept. If using an analogy that draws from general knowledge, clearly frame it as such.
--- END OF EXPLAINER INSTRUCTIONS ---

[Source Text]:
\"\"\"
{source_text}
\"\"\"

**Explanation of Main Subject:**
"""

PROMPT_EXPLANATION_VERIFICATION_FOR_LLM_B = """
**Your Role:** You are a Factual Accuracy Reviewer for AI-generated explanations.
**Your Task:**
Critically evaluate the provided [Generated Explanation] against the original [Source Text]. Your primary goal is to determine if the factual assertions within the explanation (specifically in its "What It Is," "How It Works (or Key Aspects)," and "Why It Matters" sections) are accurate and directly supported by the [Source Text].

**Inputs:**
1.  **[Source Text]:** The original document.
2.  **[Generated Explanation]:** The explanation produced by another AI. It should identify a concept and explain it in a structured format.

**Evaluation Criteria (Focus on Factual Accuracy from Source Text):**

1.  **Accuracy of "Concept" Identification:** Does the identified "Concept" in the explanation accurately reflect a main subject or core concept of the [Source Text]? (Minor issue if debatable but plausible; Major if clearly off-topic).
2.  **Accuracy of "What It Is":** Is the definition/description provided for the concept factually accurate and consistent with the [Source Text]?
3.  **Accuracy of "How It Works (or Key Aspects)":** Are the mechanisms, processes, or key aspects described factually accurate and consistent with the [Source Text]?
4.  **Accuracy of "Why It Matters":** Are the significance, use cases, or implications stated factually accurate and supported by the [Source Text]?
5.  **No Unsupported External Information:** Does the explanation (outside of a clearly marked optional analogy) avoid introducing significant factual information not present in or directly inferable from the [Source Text]?

**Output Instructions:**

* If the "Concept" is relevant, and the "What It Is," "How It Works (or Key Aspects)," and "Why It Matters" sections of the [Generated Explanation] are substantially factually accurate and well-supported by the [Source Text] (minor phrasing differences are acceptable, but no significant factual contradictions or unsupported assertions), respond with **only** the following words:
    `GREEN LIGHT`

* If the identified "Concept" is largely irrelevant to the source text, OR if there are significant factual inaccuracies, misrepresentations, or assertions not supported by the [Source Text] in the core explanation sections, respond with:
    `RED LIGHT`
    Immediately followed by a brief, bulleted list identifying the primary factual issue(s). Focus on the 1-3 most critical inaccuracies or relevance issues.

---
**Please begin your assessment.**
**[Source Text]:**
\"\"\"
{source_text}
\"\"\"
**[Generated Explanation]:**
\"\"\"
{step1_output}
\"\"\"
**Assessment Output:**
"""

PROMPT_THIELIAN_LENS_GENERATION_LLM_A = """
Read the [Source Text] carefully.
Your task is to act as a contrarian thinker, inspired by Peter Thiel's question in *Zero to One* ("What important truth do very few people agree with you on?").
Identify one meaningful and non-obvious insight from within the [Source Text] that challenges conventional wisdom or presents a viewpoint related to the text's content that few people would readily agree with. The insight must be well-grounded in the provided text, not an external opinion.

Present your finding in the following format:

**Contrarian Insight:**
[Your identified contrarian insight here, articulated clearly and concisely based on the Source Text.]

[Source Text]:
\"\"\"
{source_text}
\"\"\"

**Generated Output:**
"""

PROMPT_THIELIAN_LENS_VERIFICATION_LLM_B = """
**Your Role:** You are a Critical Reviewer tasked with evaluating an AI-generated "Contrarian Insight" based on a [Source Text].

**Your Task:**
Assess the provided [Generated Insight] against the original [Source Text] and the principles of identifying a meaningful, contrarian insight that is grounded in the text. Determine if it warrants a 'GREEN LIGHT' (a valid contrarian insight well-grounded in the text) or a 'RED LIGHT' (flawed).

**Inputs:**
1.  **[Source Text]:** The original document.
2.  **[Generated Insight]:** The output from another AI, which should be formatted with a "**Contrarian Insight:**" heading followed by the insight text.

**Evaluation Criteria:**

1.  **Grounded in Source:** Is the [Generated Insight] plausibly derived from, supported by, or a reasonable (even if contrarian) interpretation of the [Source Text]? It must not be a random statement disconnected from the document's content.
2.  **Contrarian Nature:** Does the insight genuinely challenge conventional wisdom generally associated with the topic of the [Source Text], offer a non-obvious viewpoint, or state something relevant to the text that most people might not immediately agree with or consider?
3.  **Meaningfulness & Non-Triviality:** Is the insight substantive and thought-provoking in relation to the [Source Text], rather than being a trivial observation, a nonsensical statement, or an obviously false claim merely presented as contrarian?
4.  **Clarity of Articulation:** Is the insight expressed clearly and understandably?

**Output Instructions:**

* If the [Generated Insight] meets all the following conditions:
    a) It is well-grounded in the [Source Text].
    b) It genuinely presents a contrarian/non-obvious perspective relevant to the text's content.
    c) It is meaningful and clearly articulated.
  Respond with **only** the following words:
    `GREEN LIGHT`

* Otherwise, if the [Generated Insight] has one or more significant flaws based on the criteria (e.g., not from source, not contrarian, trivial, unclear), respond with:
    `RED LIGHT`
    Immediately followed by a brief, bulleted list identifying the primary reason(s)** for the 'RED LIGHT'. Focus on the 1-3 most critical issues from the following possibilities:
    * `RED LIGHT`
        * Grounding: Insight not supported by or is irrelevant to the Source Text.
        * Contrarianism: States a common/widely accepted view, or is not genuinely contrarian in the context of the text.
        * Meaningfulness: Insight is trivial, nonsensical, or poorly articulated.
        * Validity: Insight is an extreme or unfounded claim with no reasonable basis in the text, even for a contrarian argument.
---

**Please begin your assessment.**

**[Source Text]:**
\"\"\"
{source_text}
\"\"\"

**[Generated Insight]:**
\"\"\"
{step1_output}
\"\"\"
**Assessment Output:**
"""

PROMPT_MOVEMENT_GENERATION_LLM_A = """
Read the [Source Text] carefully.
Your task is to identify an idea within the [Source Text] that you believe holds the potential to spark a larger movement.
Then, describe what that movement might look like in practice. Your description must be grounded in the seed idea from the text and should elaborate on the movement's potential goals, its core values, and the methods it might employ.

**Formatting Instructions:**
- Present your entire response under the main heading "**Potential Movement Idea:**".
- Within your description of the movement, **bold your topic sentences** to clearly signal your main claims or key aspects of the goals, values, and methods.

[Source Text]:
\"\"\"
{source_text}
\"\"\"

**Generated Output:**
"""

PROMPT_MOVEMENT_VERIFICATION_LLM_B = """
**Your Role:** You are a Critical Reviewer evaluating an AI-generated "Potential Movement Idea" based on a [Source Text].

**Your Task:**
Assess the provided [Generated Movement Idea] against the original [Source Text] and the specific requirements of the task. Determine if it warrants a 'GREEN LIGHT' (a well-grounded, plausible, and coherently described potential movement) or a 'RED LIGHT' (flawed).

**Inputs:**
1.  **[Source Text]:** The original document.
2.  **[Generated Movement Idea]:** The output from another AI, which should start with a "**Potential Movement Idea:**" heading and describe a movement including its goals, values, and methods, ideally with bolded topic sentences.

**Evaluation Criteria:**

1.  **Grounded in Source (Seed Idea):** Is the *core idea* identified to spark the movement actually present or strongly and reasonably inferable from the [Source Text]? The movement description itself is an extrapolation, but the seed idea must originate from the document.
2.  **Coherence & Completeness of Movement Description:**
    * Does the description clearly articulate the movement's potential **goals, values, and methods**?
    * Is the overall description of the movement internally consistent and understandable?
3.  **Plausibility of "Movement Potential":** Given the seed idea from the text, is the described movement a *plausible* (even if ambitious or niche) extrapolation? Does the seed idea have at least some conceivable potential to inspire collective action or a significant shift in thinking as described?
4.  **Articulation & Formatting Adherence:**
    * Is the output clearly written?
    * Was an attempt made to bold topic sentences as instructed in the original prompt for LLM A? (This is a minor point for the Green/Red light but worth noting if completely ignored).

**Output Instructions:**

* If the [Generated Movement Idea] meets all the following conditions:
    a) The seed idea is clearly rooted in the [Source Text].
    b) The described movement (including goals, values, methods) is coherent and plausible as an extrapolation.
    c) The output is well-articulated and addresses the core requirements of the generation prompt (describing goals, values, methods).
  Respond with **only** the following words:
    `GREEN LIGHT`

* Otherwise, if the [Generated Movement Idea] has one or more significant flaws based on the criteria, respond with:
    `RED LIGHT`
    Immediately followed by a brief, bulleted list identifying the primary reason(s) for the 'RED LIGHT'. Focus on the 1-3 most critical issues from the following possibilities:
    * `RED LIGHT`
        * Grounding: The seed idea for the movement is not found in or is irrelevant to the Source Text.
        * Coherence/Completeness: Fails to clearly describe goals, values, or methods, or the description is contradictory/nonsensical.
        * Plausibility: The described movement or its potential arising from the seed idea is entirely implausible or trivial.
        * Articulation: The output is very poorly articulated or fails to address main prompt requirements.
---

**Please begin your assessment.**

**[Source Text]:**
\"\"\"
{source_text}
\"\"\"

**[Generated Movement Idea]:**
\"\"\"
{step1_output}
\"\"\"
**Assessment Output:**
"""

PROMPT_LEARNING_CHUNKS_GENERATION_LLM_A = """
**Your Task:** You are an expert learning assistant. Your goal is to help someone accelerate their learning and understanding of the provided [Source Text] by breaking it down into bite-sized, memorable chunks.

**Instructions:**
1.  Read the [Source Text] carefully.
2.  Identify and extract key concepts, essential definitions, and main ideas.
3.  Transform these into concise, easily digestible bullet points. Each bullet point should represent a "learning chunk."
4.  Focus on making these chunks memorable and clear, suitable for quick review and understanding.
5.  Ensure all chunks are directly derived from the [Source Text].

**Output Format:**
Present the learning chunks as a bulleted list under the heading "**Learning Chunks:**".

[Source Text]:
\"\"\"
{source_text}
\"\"\"

**Generated Output:**
"""

PROMPT_LEARNING_CHUNKS_VERIFICATION_LLM_B = """
**Your Role:** You are a Quality Reviewer for AI-generated learning materials.
**Your Task:**
Assess the provided [Learning Chunks] against the original [Source Text] and the task requirements. Determine if they warrant a 'GREEN LIGHT' (effective learning aids) or a 'RED LIGHT' (flawed).

**Inputs:**
1.  **[Source Text]:** The original document.
2.  **[Learning Chunks]:** The bulleted list of learning chunks generated by another AI, which should start with a "**Learning Chunks:**" heading.

**Evaluation Criteria:**

1.  **Grounded in Source & Accuracy:** Are all learning chunks factually accurate and directly derived from the [Source Text]? No external information should be introduced.
2.  **Focus on Key Information:** Do the chunks effectively capture key concepts, essential definitions, and main ideas from the source? Are any critical pieces of information for understanding the core material omitted?
3.  **Conciseness & Clarity (Bite-sized):** Are the chunks genuinely "bite-sized" ‚Äì concise and to the point? Are they clearly worded and easy to understand for someone trying to learn the material?
4.  **Memorability (Plausibility):** Do the chunks seem structured in a way that could aid memorization or quick recall of important information (e.g., not overly complex, well-phrased)?
5.  **Format Adherence:** Are the learning chunks presented as a bulleted list under the specified heading as requested?

**Output Instructions:**

* If the [Learning Chunks] meet all the following conditions to a good standard:
    a) All chunks are accurately grounded in the [Source Text].
    b) They effectively cover key concepts, definitions, and main ideas.
    c) They are concise, clear, and presented as a proper bulleted list.
    d) They are plausibly structured to be memorable learning aids.
  Respond with **only** the following words:
    `GREEN LIGHT`

* Otherwise, if the [Learning Chunks] have one or more significant flaws based on the criteria, respond with:
    `RED LIGHT`
    Immediately followed by a brief, bulleted list identifying the primary reason(s) for the 'RED LIGHT'. Focus on the 1-3 most critical issues from the following possibilities:
    * `RED LIGHT`
        * Grounding/Accuracy: Chunks contain information not from the Source Text or are factually inaccurate.
        * Key Info Missing: Fails to capture essential concepts/definitions/ideas for learning.
        * Clarity/Conciseness: Chunks are too verbose, unclear, or not "bite-sized."
        * Formatting: Not presented as a proper bulleted list under the correct heading.
        * Effectiveness: Chunks are poorly structured for learning or memorability.
---

**Please begin your assessment.**

**[Source Text]:**
\"\"\"
{source_text}
\"\"\"

**[Learning Chunks]:** \"\"\"
{step1_output}
\"\"\"
**Assessment Output:**
"""

# --- Core Functions ---

def extract_text_from_pdf(uploaded_file_obj):
    """Extracts text from an uploaded PDF file object using PyMuPDF/fitz."""
    text = ""
    try:
        pdf_bytes = uploaded_file_obj.getvalue()
        with fitz.open(stream=pdf_bytes, filetype="pdf") as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                text += page.get_text("text") + "\n"
    except Exception as e:
        return None, f"Error extracting text: {e}"
    if not text.strip():
        return None, "No text content found in PDF."
    return text, None

# --- Helper functions for URL input ---
def extract_urls_from_text(raw_text):
    """Finds all unique URLs in a block of text using regex."""
    if not raw_text:
        return []
    url_pattern = re.compile(r'https?://[^\s/$.?#].[^\s]*')
    found_urls = url_pattern.findall(raw_text)
    return list(dict.fromkeys(found_urls))

def fetch_content_from_url(url):
    """Fetches and returns the text content of a URL using Jina AI Reader."""
    jina_reader_url = f"https://r.jina.ai/{url}"
    headers = {}
    if JINA_API_KEY:
        headers['Authorization'] = f'Bearer {JINA_API_KEY}'

    try:
        response = requests.get(jina_reader_url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.text, None
    except requests.exceptions.RequestException as e:
        return None, f"Error fetching URL content: {e}"

def call_openai_api(model_id, messages, temperature=0.3, max_tokens=2048):
    """Calls the OpenAI Chat Completions API."""
    if not OPENAI_API_KEY:
        return None, "OpenAI API key not found. Please set the OPENAI_API_KEY environment variable."
    try:
        client = OpenAI(api_key=OPENAI_API_KEY)
        completion = client.chat.completions.create(
            model=model_id,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        return completion.choices[0].message.content.strip(), None
    except Exception as e:
        return None, f"OpenAI API Error: {e}"

def call_groq_api(model_id, messages, temperature=0.3, max_tokens=2048):
    """Calls the Groq Chat Completions API."""
    if not GROQ_API_KEY:
        return None, "Groq API key not found. Please set the GROQ_API_KEY environment variable."
    try:
        client = Groq(api_key=GROQ_API_KEY)
        completion = client.chat.completions.create(
            model=model_id,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        return completion.choices[0].message.content.strip(), None
    except Exception as e:
        return None, f"Groq API Error: {e}"

def get_llm_response(source_text, prompt_template_str, llm_choice_name, step1_output_for_step2=None):
    """
    Prepares the prompt and calls the appropriate LLM API based on user selection.
    """
    model_info = MODEL_OPTIONS.get(llm_choice_name)
    if not model_info:
        return None, f"Invalid LLM choice: {llm_choice_name}"

    model_id = model_info["id"]
    provider = model_info["provider"]

    if step1_output_for_step2:
        filled_prompt = prompt_template_str.format(source_text=source_text, step1_output=step1_output_for_step2)
    else:
        filled_prompt = prompt_template_str.format(source_text=source_text)

    messages_for_api = [{"role": "user", "content": filled_prompt}]

    api_temperature = 0.3
    if any(keyword in prompt_template_str.lower() for keyword in ["explain", "explainer", "insight", "movement", "chunks"]):
        api_max_tokens = 2048
    else:
        api_max_tokens = 1024

    if provider == "openai":
        return call_openai_api(model_id, messages_for_api, temperature=api_temperature, max_tokens=api_max_tokens)
    elif provider == "groq":
        return call_groq_api(model_id, messages_for_api, temperature=api_temperature, max_tokens=api_max_tokens)
    else:
        return None, f"Provider '{provider}' not configured for LLM choice '{llm_choice_name}'."

# --- Password Protection ---
def check_password():
    """
    Checks if the user is authenticated. If a password is set in the environment,
    it displays a login form. Otherwise, it grants access.
    """
    if not APP_PASSWORD:
        st.session_state.authenticated = True
        return True

    if "authenticated" not in st.session_state:
        st.session_state.authenticated = False

    if st.session_state.authenticated:
        return True

    with st.form("password_form"):
        st.markdown("### üîí Password Required")
        password_attempt = st.text_input("Enter Password:", type="password", key="password_input_field")
        login_button = st.form_submit_button("Login")

        if login_button:
            if password_attempt == APP_PASSWORD:
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("Incorrect password. Please try again.")
                st.session_state.authenticated = False
    return False

# --- Main Application Logic ---
def run_app():
    """
    The main function that sets up the Streamlit page and handles the application flow.
    """
    st.set_page_config(layout="wide", page_title="Readhackers")
    st.title("üöÄ Readhackers")
    st.markdown("Process content from PDFs, URLs, or pasted text using your choice of LLMs.")

    if 'results' not in st.session_state: st.session_state.results = []
    if 'run_processing_flag' not in st.session_state: st.session_state.run_processing_flag = False
    if 'ui_task_selection' not in st.session_state: st.session_state.ui_task_selection = DEFAULT_TASK
    if 'task_for_run' not in st.session_state: st.session_state.task_for_run = DEFAULT_TASK
    if 'llm_a_for_run' not in st.session_state: st.session_state.llm_a_for_run = DEFAULT_LLM_A_NAME
    if 'llm_b_for_run' not in st.session_state: st.session_state.llm_b_for_run = DEFAULT_LLM_B_NAME
    if 'input_method' not in st.session_state: st.session_state.input_method = "Upload PDF"

    with st.sidebar:
        st.header("‚öôÔ∏è Controls")

        st.session_state.input_method = st.radio(
            "Choose your input method:",
            ("Upload PDF", "Enter URLs", "Paste Text"),
            key="input_method_selector"
        )

        uploaded_files = []
        url_input_text = ""
        pasted_text = ""

        if st.session_state.input_method == "Upload PDF":
            uploaded_files = st.file_uploader("Upload PDF files", type="pdf", accept_multiple_files=True, key="pdf_uploader")
        elif st.session_state.input_method == "Enter URLs":
            url_input_text = st.text_area("Enter one or more URLs (one per line):", height=150, key="url_input")
        elif st.session_state.input_method == "Paste Text":
            pasted_text = st.text_area("Paste your text here:", height=150, key="text_input")

        def on_task_change_callback():
            if st.session_state.ui_task_selector != st.session_state.task_for_run :
                st.session_state.results = []
                st.session_state.run_processing_flag = False

        st.session_state.ui_task_selection = st.selectbox(
            "Select Task:", options=TASK_OPTIONS,
            index=TASK_OPTIONS.index(st.session_state.ui_task_selection),
            on_change=on_task_change_callback, key="ui_task_selector",
            help="Choose the operation to perform on the content."
        )

        st.subheader("Select LLMs for Processing:")
        current_llm_a_selection = st.selectbox(
            f"Step 1: {st.session_state.ui_task_selection} with:",
            options=list(MODEL_OPTIONS.keys()),
            index=list(MODEL_OPTIONS.keys()).index(st.session_state.get('llm_a_for_run', DEFAULT_LLM_A_NAME)) \
                if st.session_state.get('llm_a_for_run', DEFAULT_LLM_A_NAME) in MODEL_OPTIONS else 0,
            key="ui_llm_a_selector",
            help="Choose LLM for the main generation task."
        )
        current_llm_b_selection = st.selectbox(
            "Step 2: Verify Output with:",
            options=list(MODEL_OPTIONS.keys()),
            index=list(MODEL_OPTIONS.keys()).index(st.session_state.get('llm_b_for_run', DEFAULT_LLM_B_NAME)) \
                if st.session_state.get('llm_b_for_run', DEFAULT_LLM_B_NAME) in MODEL_OPTIONS else 0,
            key="ui_llm_b_selector", help="Choose LLM for the verification step."
        )

        is_input_provided = uploaded_files or url_input_text.strip() or pasted_text.strip()
        if st.button(f"Process {st.session_state.input_method}", type="primary", disabled=not is_input_provided):
            st.session_state.results = []
            st.session_state.run_processing_flag = True
            st.session_state.task_for_run = st.session_state.ui_task_selection
            st.session_state.llm_a_for_run = current_llm_a_selection
            st.session_state.llm_b_for_run = current_llm_b_selection
            st.rerun()

        if st.session_state.results:
            # --- FIXED: Clear Results button logic ---
            if st.button("Clear Results"):
                st.session_state.results = []
                st.session_state.run_processing_flag = False
                
                # Safely clear text-based inputs
                if 'url_input' in st.session_state:
                    st.session_state.url_input = ""
                if 'text_input' in st.session_state:
                    st.session_state.text_input = ""
                
                # The file uploader does not need to be (and cannot be) cleared this way.
                # A rerun is sufficient to reset the processing state.
                st.rerun()

    if st.session_state.get('run_processing_flag', False):
        input_queue = []
        if st.session_state.input_method == "Upload PDF":
            input_queue = [{"identifier": f.name, "data": f, "type": "pdf"} for f in uploaded_files]
        elif st.session_state.input_method == "Enter URLs":
            urls = extract_urls_from_text(url_input_text)
            input_queue = [{"identifier": url, "data": url, "type": "url"} for url in urls]
        elif st.session_state.input_method == "Paste Text":
            if pasted_text.strip():
                input_queue = [{"identifier": "Pasted Text", "data": pasted_text, "type": "text"}]

        if not input_queue:
            st.warning("No valid input found to process. Please provide input and try again.")
            st.session_state.run_processing_flag = False
            st.rerun()

        api_keys_ok = True
        providers_in_use = set()
        if st.session_state.llm_a_for_run in MODEL_OPTIONS:
            providers_in_use.add(MODEL_OPTIONS[st.session_state.llm_a_for_run]["provider"])
        if st.session_state.llm_b_for_run in MODEL_OPTIONS:
            providers_in_use.add(MODEL_OPTIONS[st.session_state.llm_b_for_run]["provider"])

        if "groq" in providers_in_use and not GROQ_API_KEY:
            st.error("Groq API key is missing..."); api_keys_ok = False
        if "openai" in providers_in_use and not OPENAI_API_KEY:
            st.error("OpenAI API key is missing..."); api_keys_ok = False

        if not api_keys_ok:
            st.warning("Processing cannot start due to missing API keys.")
            st.session_state.run_processing_flag = False
        else:
            total_items = len(input_queue)
            progress_bar_placeholder = st.empty(); status_text_placeholder = st.empty()
            task_for_this_run = st.session_state.task_for_run
            llm_a_for_this_run = st.session_state.llm_a_for_run
            llm_b_for_this_run = st.session_state.llm_b_for_run

            with st.spinner(f"Processing {total_items} item(s) for '{task_for_this_run}' task..."):
                for i, item in enumerate(input_queue):
                    identifier = item['identifier']
                    current_file_result = {
                        "filename": identifier, "task_performed": task_for_this_run,
                        "llm_a_used": llm_a_for_this_run, "llm_b_used": llm_b_for_this_run,
                        "step1_output": None, "step2_verification": None, "error_message": None,
                        "extracted_text_snippet": None, "time_step1_sec": None, "time_step2_sec": None
                    }
                    progress_percentage = (i + 1) / total_items
                    progress_bar_placeholder.progress(progress_percentage)
                    status_text_placeholder.text(f"Processing item {i+1}/{total_items}: {identifier}...")

                    try:
                        source_text, extraction_error = None, None
                        if item['type'] == 'pdf':
                            source_text, extraction_error = extract_text_from_pdf(item['data'])
                        elif item['type'] == 'url':
                            source_text, extraction_error = fetch_content_from_url(item['data'])
                        elif item['type'] == 'text':
                            source_text, extraction_error = item['data'], None

                        if extraction_error:
                            current_file_result["error_message"] = extraction_error
                            st.session_state.results.append(current_file_result)
                            continue
                        
                        if not source_text or not source_text.strip():
                            current_file_result["error_message"] = "Extracted content is empty."
                            st.session_state.results.append(current_file_result)
                            continue

                        current_file_result["extracted_text_snippet"] = (source_text[:500] + "...") if len(source_text) > 500 else source_text

                        if task_for_this_run == TASK_PARAGRAPH_SUMMARY:
                            prompt_step1_template, prompt_step2_template = PROMPT_SUMMARY_GENERATION, PROMPT_SUMMARY_VERIFICATION
                        elif task_for_this_run == TASK_EXPLAIN_MAIN_SUBJECT:
                            prompt_step1_template, prompt_step2_template = PROMPT_EXPLAINER_TASK_FOR_LLM_A, PROMPT_EXPLANATION_VERIFICATION_FOR_LLM_B
                        elif task_for_this_run == TASK_THIELIAN_LENS:
                            prompt_step1_template, prompt_step2_template = PROMPT_THIELIAN_LENS_GENERATION_LLM_A, PROMPT_THIELIAN_LENS_VERIFICATION_LLM_B
                        elif task_for_this_run == TASK_IDENTIFY_MOVEMENT:
                            prompt_step1_template, prompt_step2_template = PROMPT_MOVEMENT_GENERATION_LLM_A, PROMPT_MOVEMENT_VERIFICATION_LLM_B
                        elif task_for_this_run == TASK_LEARNING_CHUNKS:
                            prompt_step1_template, prompt_step2_template = PROMPT_LEARNING_CHUNKS_GENERATION_LLM_A, PROMPT_LEARNING_CHUNKS_VERIFICATION_LLM_B
                        else:
                            current_file_result["error_message"] = f"Invalid task: {task_for_this_run}"
                            st.session_state.results.append(current_file_result)
                            continue

                        status_text_placeholder.text(f"Step 1 ({task_for_this_run}) for: {identifier}...")
                        start_time_step1 = time.perf_counter()
                        step1_output, step1_error = get_llm_response(source_text, prompt_step1_template, llm_a_for_this_run)
                        current_file_result["time_step1_sec"] = round(time.perf_counter() - start_time_step1, 2)
                        if step1_error:
                            current_file_result["error_message"] = f"Step 1 error: {step1_error}"
                            st.session_state.results.append(current_file_result); continue
                        current_file_result["step1_output"] = step1_output

                        status_text_placeholder.text(f"Step 2 (Verification) for: {identifier}...")
                        start_time_step2 = time.perf_counter()
                        verification_output, verification_error = get_llm_response(source_text, prompt_step2_template, llm_b_for_this_run, step1_output_for_step2=step1_output)
                        current_file_result["time_step2_sec"] = round(time.perf_counter() - start_time_step2, 2)
                        if verification_error:
                            current_file_result["error_message"] = f"Step 2 error: {verification_error}"
                            current_file_result["step2_verification"] = "Verification failed."
                            st.session_state.results.append(current_file_result); continue
                        current_file_result["step2_verification"] = verification_output

                    except Exception as e:
                        current_file_result["error_message"] = f"An unexpected error occurred: {str(e)}"
                    st.session_state.results.append(current_file_result)

                status_text_placeholder.success(f"All {total_items} item(s) processed for '{task_for_this_run}' task!")
                progress_bar_placeholder.empty()
            st.session_state.run_processing_flag = False

    if st.session_state.results:
        st.markdown("---")
        st.header("üìä Processing Results")
        downloadable_content = ""
        for i, res in enumerate(st.session_state.results):
            task_label = res.get('task_performed', "N/A")
            llm_a_used = res.get('llm_a_used', "N/A")
            llm_b_used = res.get('llm_b_used', "N/A")
            expander_title = f"Results for: {res['filename']} (Task: {task_label})"
            if res["error_message"]: expander_title += " (‚ö†Ô∏è Error)"

            with st.expander(expander_title, expanded=False):
                downloadable_content += f"--- Results for: {res['filename']} ---\nTask: {task_label}\n\n"
                if res["error_message"]:
                    st.error(f"Error: {res['error_message']}")
                    downloadable_content += f"Error: {res['error_message']}\n\n"

                header_text = f"Step 1 Output by {llm_a_used}"
                if task_label == TASK_PARAGRAPH_SUMMARY: header_text = f"Summary by {llm_a_used}"
                elif task_label == TASK_EXPLAIN_MAIN_SUBJECT: header_text = f"Explanation by {llm_a_used}"
                elif task_label == TASK_THIELIAN_LENS: header_text = f"Thielian Insight by {llm_a_used}"
                elif task_label == TASK_IDENTIFY_MOVEMENT: header_text = f"Potential Movement Idea by {llm_a_used}"
                elif task_label == TASK_LEARNING_CHUNKS: header_text = f"Learning Chunks by {llm_a_used}"

                if res["step1_output"]:
                    st.subheader(header_text)
                    if res["time_step1_sec"] is not None:
                        st.caption(f"Time: {res['time_step1_sec']:.2f}s")
                        downloadable_content += f"Time: {res['time_step1_sec']:.2f}s\n"
                    st.markdown(res["step1_output"])
                    downloadable_content += f"{header_text}:\n{res['step1_output']}\n\n"
                elif not res["error_message"]:
                    st.info("Step 1 output was not generated.")
                    downloadable_content += "Step 1 output N/A.\n\n"

                if res["step2_verification"]:
                    st.subheader(f"Step 2 Verification by {llm_b_used}:")
                    if res["time_step2_sec"] is not None:
                        st.caption(f"Time: {res['time_step2_sec']:.2f}s")
                        downloadable_content += f"Time: {res['time_step2_sec']:.2f}s\n"
                    downloadable_content += f"Verification by {llm_b_used}:\n"

                    if "GREEN LIGHT" in res["step2_verification"].upper():
                        st.success("‚úÖ GREEN LIGHT")
                        downloadable_content += "GREEN LIGHT\n"
                    elif "RED LIGHT" in res["step2_verification"].upper():
                        st.error("‚ùå RED LIGHT")
                        downloadable_content += "RED LIGHT\n"
                        try:
                            reasons = res["step2_verification"].upper().split("RED LIGHT", 1)[1].strip()
                            if reasons:
                                st.markdown("**Reasons:**")
                                downloadable_content += "Reasons:\n"
                                for line in reasons.split('\n'):
                                    clean_line = line.strip()
                                    if clean_line.startswith("*") or clean_line.startswith("-"):
                                        st.markdown(clean_line)
                                        downloadable_content += f"{clean_line}\n"
                                    elif clean_line:
                                        st.markdown(f"* {clean_line}")
                                        downloadable_content += f"* {clean_line}\n"
                        except IndexError:
                            pass
                    else:
                        st.info(res["step2_verification"])
                        downloadable_content += f"{res['step2_verification']}\n"
                    downloadable_content += "\n"
                elif not res["error_message"] and res["step1_output"]:
                    st.info("Step 2 verification was not generated.")
                    downloadable_content += "Step 2 N/A.\n\n"
                downloadable_content += "\n"

        if downloadable_content:
            st_copy_to_clipboard(downloadable_content, "Copy All Results to Clipboard")
            st.caption("Click the button above to copy all expanded results to your clipboard.")

    st.markdown("---")
    st.caption("Ensure API keys (GROQ_API_KEY, OPENAI_API_KEY, JINA_API_KEY) and an optional PASSWORD are set as environment variables.")
    st.caption(f"Available Models: {', '.join(MODEL_OPTIONS.keys())}")
    st.caption("This app requires `streamlit`, `PyMuPDF`, `openai`, `groq`, `requests`, and `streamlit-copy-to-clipboard`.")


# --- Main execution ---
if __name__ == "__main__":
    if check_password():
        run_app()

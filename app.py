import streamlit as st
import os
import fitz  # PyMuPDF
from openai import OpenAI
from groq import Groq
from st_copy_to_clipboard import st_copy_to_clipboard
import time

# --- Configuration & Constants ---

# API Key Retrieval
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

# --- IMPORTANT: Replace these with actual API model identifiers ---
MODEL_OPTIONS = {
    "Llama-3.3-70B (Groq)": {"id": "llama-3.3-70b-versatile", "provider": "groq"},
    "GPT-4.1 (OpenAI)": {"id": "gpt-4.1", "provider": "openai"}, # CRITICAL: Replace "gpt-4.1" with a valid OpenAI model ID like "gpt-4o" or "gpt-4-turbo"
    "Llama-3.1-8B-Instant (Groq)": {"id": "llama-3.1-8b-instant", "provider": "groq"}
}
DEFAULT_LLM_A_NAME = "Llama-3.3-70B (Groq)"
DEFAULT_LLM_B_NAME = "GPT-4.1 (OpenAI)"

# Task Definitions
TASK_PARAGRAPH_SUMMARY = "Paragraph Summary"
TASK_EXPLAIN_MAIN_SUBJECT = "Explain Main Subject"
TASK_THIELIAN_LENS = "Thielian Lens Insight"
TASK_OPTIONS = [TASK_PARAGRAPH_SUMMARY, TASK_EXPLAIN_MAIN_SUBJECT, TASK_THIELIAN_LENS]
DEFAULT_TASK = TASK_PARAGRAPH_SUMMARY

# Prompts
PROMPT_SUMMARY_GENERATION = """
**Your Task:** Generate a high-quality, concise, and comprehensive paragraph summary of the provided [Source Text].

**Instructions for Crafting reputed Paragraph Summary:**
1.  **Understand Thoroughly:** Read the [Source Text] carefully to identify its central theme, main arguments, key findings, and essential supporting information.
2.  **Synthesize, Don't Just Extract:** Your summary should be a new piece of writing that expresses the core ideas of the source in your own words (as an LLM). Avoid simply copying and pasting sentences from the original text. The goal is true synthesis.
3.  **Accuracy is Paramount:** Ensure the summary accurately reflects the meaning and factual content of the [Source Text]. Do not introduce any information, interpretations, or opinions not explicitly present in the original.
4.  **Focus on Key Information:** Identify and include only the most important points necessary to convey the essence of the source. Omit minor details, redundant information, or tangential discussions.
5.  **Single, Coherent Paragraph:** The entire summary must be presented as a single, well-structured paragraph. It should flow logically, with clear transitions between ideas, making it easy for someone to understand the core message without having read the original text.
6.  **Conciseness:** Be as brief as possible while still covering the essential information. Every sentence should contribute to the summary's purpose.
7.  **Neutral and Objective Tone:** Unless the source text itself has a highly subjective tone that is central to its message, maintain a neutral and objective perspective.

**[Source Text]:**
\"\"\"
{source_text}
\"\"\"

**Paragraph Summary:**
"""

PROMPT_SUMMARY_VERIFICATION = """
**Your Role:** You are a meticulous Quality Assurance Editor, specializing in the evaluation of paragraph summaries.
**Your Task:**
Your goal is to assess the provided [Paragraph Summary] against the original [Source Text] to determine if it is a "good" summary according to the criteria below. Based on this assessment, you will output either a 'GREEN LIGHT' (if the summary is satisfactory) or a 'RED LIGHT' (if it has significant issues).

**Inputs:**
1.  **[Source Text]:** The full original text from which the summary was derived.
2.  **[Paragraph Summary]:** The single paragraph summary generated by another AI.

**Evaluation Criteria for a "Good Paragraph Summary":**
1.  **Accuracy:**
    * Does the summary faithfully represent the core information and meaning of the [Source Text]?
    * Does it avoid introducing any factual errors, new information not present in the source, or misrepresentations?
2.  **Key Information Coverage (Comprehensiveness for a Paragraph):**
    * Does the summary successfully capture the main idea(s) and the most crucial supporting points of the [Source Text] that are essential for understanding its essence?
    * Are any critical aspects or the central message of the source omitted, making the summary incomplete in its representation of the core content?
3.  **Conciseness and Focus:**
    * Is the summary appropriately concise and focused, avoiding unnecessary details, fluff, or redundant information that doesn't contribute to the main points?
    * Is it well-contained within a single paragraph without feeling overly dense or rushed?
4.  **Coherence and Readability:**
    * Is the paragraph well-written, with ideas flowing logically and sentences connecting smoothly?
    * Is it easy to understand for someone who has not read the original [Source Text]?
5.  **Neutrality and Objectivity:**
    * Does the summary maintain a neutral and objective tone, reflecting the information as presented in the [Source Text] (unless the source itself is clearly subjective and the summary needs to reflect that specific tone)?
6.  **Format Adherence:**
    * Is the summary correctly presented as a single, coherent paragraph?

**Output Instructions:**
* If the [Paragraph Summary] substantially meets **all** the above criteria to a good standard (minor stylistic preferences or very trivial, non-critical omissions might be acceptable, but there should be no significant flaws), respond with **only** the following words:
    `GREEN LIGHT`
* If the [Paragraph Summary] has **one or more significant flaws** in any of the evaluated criteria (especially concerning Accuracy or Key Information Coverage), respond with:
    `RED LIGHT`
    Immediately followed by a brief, bulleted list identifying the **primary reason(s)** for the 'RED LIGHT' (focus on the 1-3 most critical issues).

---
**Please begin your assessment.**
**[Source Text]:**
\"\"\"
{source_text}
\"\"\"
**[Paragraph Summary]:** \"\"\"
{step1_output}
\"\"\"
**Assessment Output:**
"""

PROMPT_EXPLAINER_TASK_FOR_LLM_A = """
You will be provided with a [Source Text]. Your first task is to identify the main subject or core concept discussed within this document.
Once you have identified this main subject/core concept, your second task is to explain it comprehensively based *only* on the information present in the [Source Text].
Use the following detailed structure and instructions for your explanation:

--- START OF EXPLAINER INSTRUCTIONS ---
You are an expert explainer. Your task is to break down the concept provided (which is the main subject of the document you've read) into clear, understandable parts for someone who is curious but not necessarily an expert.

# Instructions for the Explanation:
- Your explanation must be based on the information found within the [Source Text]. Do not introduce external knowledge unless it's for a general analogy clearly marked as such.
- Explain it step by step, avoiding jargon unless necessary and present in the [Source Text].
- If you use any technical terms from the [Source Text], define them simply based on their context in the document.
- Use analogies or metaphors where helpful to clarify concepts found in the text.
- Highlight not just *what* the main subject is, but *how* and *why* it works or is significant, according to the [Source Text].
- Structure your explanation clearly.

# Output Format for the Explanation:
**Concept:** [State the main subject/core concept you have identified from the Source Text here]

**What It Is:**
A simple definition of this concept, based on the Source Text.

**How It Works (or Key Aspects):**
A step-by-step explanation, mechanism, or key aspects of this concept, based on the Source Text.

**Why It Matters (according to the Source Text):**
Explain the significance, use cases, or implications of this concept as indicated in the Source Text.

**Analogy (Optional, if applicable and helps clarify):**
Use a metaphor or real-world comparison to help visualize the concept. If using an analogy that draws from general knowledge, clearly frame it as such.
--- END OF EXPLAINER INSTRUCTIONS ---

[Source Text]:
\"\"\"
{source_text}
\"\"\"

**Explanation of Main Subject:**
"""

PROMPT_EXPLANATION_VERIFICATION_FOR_LLM_B = """
**Your Role:** You are a Factual Accuracy Reviewer for AI-generated explanations.
**Your Task:**
Critically evaluate the provided [Generated Explanation] against the original [Source Text]. Your primary goal is to determine if the factual assertions within the explanation (specifically in its "What It Is," "How It Works (or Key Aspects)," and "Why It Matters" sections) are accurate and directly supported by the [Source Text].

**Inputs:**
1.  **[Source Text]:** The original document.
2.  **[Generated Explanation]:** The explanation produced by another AI. It should identify a concept and explain it in a structured format.

**Evaluation Criteria (Focus on Factual Accuracy from Source Text):**

1.  **Accuracy of "Concept" Identification:** Does the identified "Concept" in the explanation accurately reflect a main subject or core concept of the [Source Text]? (Minor issue if debatable but plausible; Major if clearly off-topic).
2.  **Accuracy of "What It Is":** Is the definition/description provided for the concept factually accurate and consistent with the [Source Text]?
3.  **Accuracy of "How It Works (or Key Aspects)":** Are the mechanisms, processes, or key aspects described factually accurate and consistent with the [Source Text]?
4.  **Accuracy of "Why It Matters":** Are the significance, use cases, or implications stated factually accurate and supported by the [Source Text]?
5.  **No Unsupported External Information:** Does the explanation (outside of a clearly marked optional analogy) avoid introducing significant factual information not present in or directly inferable from the [Source Text]?

**Output Instructions:**

* If the "Concept" is relevant, and the "What It Is," "How It Works (or Key Aspects)," and "Why It Matters" sections of the [Generated Explanation] are substantially factually accurate and well-supported by the [Source Text] (minor phrasing differences are acceptable, but no significant factual contradictions or unsupported assertions), respond with **only** the following words:
    `GREEN LIGHT`

* If the identified "Concept" is largely irrelevant to the source text, OR if there are significant factual inaccuracies, misrepresentations, or assertions not supported by the [Source Text] in the core explanation sections, respond with:
    `RED LIGHT`
    Immediately followed by a brief, bulleted list identifying the primary factual issue(s). Focus on the 1-3 most critical inaccuracies or relevance issues.

---
**Please begin your assessment.**
**[Source Text]:**
\"\"\"
{source_text}
\"\"\"
**[Generated Explanation]:**
\"\"\"
{step1_output} 
\"\"\"
**Assessment Output:**
"""

PROMPT_THIELIAN_LENS_GENERATION_LLM_A = """
Read the [Source Text] carefully.
Your task is to act as a contrarian thinker, inspired by Peter Thiel's question in *Zero to One* ("What important truth do very few people agree with you on?").
Identify one meaningful and non-obvious insight from within the [Source Text] that challenges conventional wisdom or presents a viewpoint related to the text's content that few people would readily agree with. The insight must be well-grounded in the provided text, not an external opinion.

Present your finding in the following format:

**Contrarian Insight:**
[Your identified contrarian insight here, articulated clearly and concisely based on the Source Text.]

[Source Text]:
\"\"\"
{source_text}
\"\"\"

**Generated Output:**
"""

PROMPT_THIELIAN_LENS_VERIFICATION_LLM_B = """
**Your Role:** You are a Critical Reviewer tasked with evaluating an AI-generated "Contrarian Insight" based on a [Source Text].

**Your Task:**
Assess the provided [Generated Insight] against the original [Source Text] and the principles of identifying a meaningful, contrarian insight that is grounded in the text. Determine if it warrants a 'GREEN LIGHT' (a valid contrarian insight well-grounded in the text) or a 'RED LIGHT' (flawed).

**Inputs:**
1.  **[Source Text]:** The original document.
2.  **[Generated Insight]:** The output from another AI, which should be formatted with a "**Contrarian Insight:**" heading followed by the insight text.

**Evaluation Criteria:**

1.  **Grounded in Source:** Is the [Generated Insight] plausibly derived from, supported by, or a reasonable (even if contrarian) interpretation of the [Source Text]? It must not be a random statement disconnected from the document's content.
2.  **Contrarian Nature:** Does the insight genuinely challenge conventional wisdom generally associated with the topic of the [Source Text], offer a non-obvious viewpoint, or state something relevant to the text that most people might not immediately agree with or consider?
3.  **Meaningfulness & Non-Triviality:** Is the insight substantive and thought-provoking in relation to the [Source Text], rather than being a trivial observation, a nonsensical statement, or an obviously false claim merely presented as contrarian?
4.  **Clarity of Articulation:** Is the insight expressed clearly and understandably?

**Output Instructions:**

* If the [Generated Insight] meets all the following conditions:
    a) It is well-grounded in the [Source Text].
    b) It genuinely presents a contrarian/non-obvious perspective relevant to the text's content.
    c) It is meaningful and clearly articulated.
  Respond with **only** the following words:
    `GREEN LIGHT`

* Otherwise, if the [Generated Insight] has one or more significant flaws based on the criteria (e.g., not from source, not contrarian, trivial, unclear), respond with:
    `RED LIGHT`
    Immediately followed by a brief, bulleted list identifying the primary reason(s) for the 'RED LIGHT'. Focus on the 1-3 most critical issues from the following possibilities:
    * `RED LIGHT`
        * Grounding: Insight not supported by or is irrelevant to the Source Text.
        * Contrarianism: States a common/widely accepted view, or is not genuinely contrarian in the context of the text.
        * Meaningfulness: Insight is trivial, nonsensical, or poorly articulated.
        * Validity: Insight is an extreme or unfounded claim with no reasonable basis in the text, even for a contrarian argument.
---

**Please begin your assessment.**

**[Source Text]:**
\"\"\"
{source_text}
\"\"\"

**[Generated Insight]:**
\"\"\"
{step1_output} 
\"\"\" 
**Assessment Output:**
"""

# --- Core Functions ---

def extract_text_from_pdf(uploaded_file_obj):
    """Extracts text from an uploaded PDF file object using PyMuPDF/fitz."""
    text = ""
    try:
        pdf_bytes = uploaded_file_obj.getvalue()
        with fitz.open(stream=pdf_bytes, filetype="pdf") as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                text += page.get_text("text") + "\n" 
    except Exception as e:
        return None, f"Error extracting text: {e}"
    if not text.strip():
        return None, "No text content found in PDF."
    return text, None

def call_openai_api(model_id, messages, temperature=0.3, max_tokens=2048):
    """Helper function to call OpenAI API using client.chat.completions.create."""
    if not OPENAI_API_KEY:
        return None, "OpenAI API key not found. Please set the OPENAI_API_KEY environment variable."
    try:
        client = OpenAI(api_key=OPENAI_API_KEY)
        completion = client.chat.completions.create(
            model=model_id,
            messages=messages,
            temperature=temperature, 
            max_tokens=max_tokens    
        )
        return completion.choices[0].message.content.strip(), None
    except Exception as e:
        return None, f"OpenAI API Error (using chat.completions.create): {e}"

def call_groq_api(model_id, messages, temperature=0.3, max_tokens=2048):
    """Helper function to call Groq API."""
    if not GROQ_API_KEY:
        return None, "Groq API key not found. Please set the GROQ_API_KEY environment variable."
    try:
        client = Groq(api_key=GROQ_API_KEY)
        completion = client.chat.completions.create(
            model=model_id,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        return completion.choices[0].message.content.strip(), None
    except Exception as e:
        return None, f"Groq API Error: {e}"

def get_llm_response(source_text, prompt_template_str, llm_choice_name, step1_output_for_step2=None):
    """
    Gets response from the selected LLM.
    Returns (response_text, error_message)
    """
    model_info = MODEL_OPTIONS.get(llm_choice_name)
    if not model_info:
        return None, f"Invalid LLM choice: {llm_choice_name}"

    model_id = model_info["id"]
    provider = model_info["provider"]

    if step1_output_for_step2: 
        filled_prompt = prompt_template_str.format(source_text=source_text, step1_output=step1_output_for_step2)
    else: 
        filled_prompt = prompt_template_str.format(source_text=source_text)
    
    messages_for_api = [{"role": "user", "content": filled_prompt}]
    
    api_temperature = 0.3 
    if "explain" in prompt_template_str.lower() or "explainer" in prompt_template_str.lower() or "insight" in prompt_template_str.lower() :
        api_max_tokens = 2048 
    else: # Default for summaries
        api_max_tokens = 1024 

    if provider == "openai":
        return call_openai_api(model_id, messages_for_api, temperature=api_temperature, max_tokens=api_max_tokens)
    elif provider == "groq":
        return call_groq_api(model_id, messages_for_api, temperature=api_temperature, max_tokens=api_max_tokens)
    else:
        return None, f"Provider '{provider}' not configured for LLM choice '{llm_choice_name}'."

# --- Streamlit UI ---

st.set_page_config(layout="wide", page_title="Two-Step PDF Processor")
st.title("üìë Two-Step PDF Processor")
st.markdown("Upload PDFs, choose a task, generate output, and verify it using your choice of LLMs.")

# Initialize session state variables if they don't exist
if 'results' not in st.session_state:
    st.session_state.results = []
if 'run_processing_flag' not in st.session_state: # Flag to trigger processing on button click
    st.session_state.run_processing_flag = False
if 'ui_task_selection' not in st.session_state: # Stores current UI selection for task
    st.session_state.ui_task_selection = DEFAULT_TASK
# These store the settings *used for the last initiated run*
if 'task_for_run' not in st.session_state:
    st.session_state.task_for_run = DEFAULT_TASK
if 'llm_a_for_run' not in st.session_state:
    st.session_state.llm_a_for_run = DEFAULT_LLM_A_NAME
if 'llm_b_for_run' not in st.session_state:
    st.session_state.llm_b_for_run = DEFAULT_LLM_B_NAME


with st.sidebar:
    st.header("‚öôÔ∏è Controls")
    uploaded_files = st.file_uploader(
        "Upload PDF files",
        type="pdf",
        accept_multiple_files=True,
        help="Upload one or more PDF files."
    )

    def on_task_change_callback():
        # If task selection changes in UI, clear previous results and reset processing trigger
        # The actual st.session_state.ui_task_selection will be updated by Streamlit's widget state
        if st.session_state.ui_task_selector != st.session_state.task_for_run : # Check if it really changed from last run's task
            st.session_state.results = []
            st.session_state.run_processing_flag = False
            # task_for_run will be updated by the selectbox itself via its key

    # UI Task Selector - its value is stored in st.session_state.ui_task_selector by Streamlit
    st.session_state.ui_task_selection = st.selectbox(
        "Select Task:",
        options=TASK_OPTIONS,
        index=TASK_OPTIONS.index(st.session_state.ui_task_selection), 
        on_change=on_task_change_callback,
        key="ui_task_selector", # Explicit key for the widget
        help="Choose the operation to perform on the PDFs."
    )

    st.subheader("Select LLMs for Processing:")
    # LLM selectors now use the current UI task selection for their labels
    # Their actual values will be snapshotted when "Process" button is clicked
    current_llm_a_selection = st.selectbox(
        f"Step 1: {st.session_state.ui_task_selection} with:",
        options=list(MODEL_OPTIONS.keys()),
        index=list(MODEL_OPTIONS.keys()).index(st.session_state.get('llm_a_for_run', DEFAULT_LLM_A_NAME)) \
            if st.session_state.get('llm_a_for_run', DEFAULT_LLM_A_NAME) in MODEL_OPTIONS else 0,
        key="ui_llm_a_selector",
        help=f"Choose the LLM to perform '{st.session_state.ui_task_selection}'."
    )
    current_llm_b_selection = st.selectbox(
        "Step 2: Verify Output with:",
        options=list(MODEL_OPTIONS.keys()),
        index=list(MODEL_OPTIONS.keys()).index(st.session_state.get('llm_b_for_run', DEFAULT_LLM_B_NAME)) \
            if st.session_state.get('llm_b_for_run', DEFAULT_LLM_B_NAME) in MODEL_OPTIONS else 0,
        key="ui_llm_b_selector",
        help="Choose the LLM to verify the output from Step 1."
    )

    if st.button(f"Process Uploaded PDFs with '{st.session_state.ui_task_selection}' Task", type="primary", disabled=not uploaded_files):
        st.session_state.results = [] 
        st.session_state.run_processing_flag = True
        # Snapshot the settings for this run
        st.session_state.task_for_run = st.session_state.ui_task_selection
        st.session_state.llm_a_for_run = current_llm_a_selection
        st.session_state.llm_b_for_run = current_llm_b_selection
        st.rerun() # Rerun to ensure the main processing block is entered with the flag set
    
    if st.session_state.results: 
        if st.button("Clear Results & Files"):
            st.session_state.results = []
            st.session_state.run_processing_flag = False
            # Reset task_for_run to default or current UI selection if needed
            # st.session_state.ui_task_selection = DEFAULT_TASK # Reset UI task to default
            # st.session_state.task_for_run = DEFAULT_TASK
            st.rerun()

if st.session_state.get('run_processing_flag', False) and uploaded_files:
    api_keys_ok = True
    # Use the snapshotted LLM choices for API key checks
    providers_in_use = set()
    if st.session_state.llm_a_for_run in MODEL_OPTIONS: 
        providers_in_use.add(MODEL_OPTIONS[st.session_state.llm_a_for_run]["provider"])
    if st.session_state.llm_b_for_run in MODEL_OPTIONS: 
        providers_in_use.add(MODEL_OPTIONS[st.session_state.llm_b_for_run]["provider"])

    if "groq" in providers_in_use and not GROQ_API_KEY:
        st.error("Groq API key is missing. Please set the GROQ_API_KEY environment variable.")
        api_keys_ok = False
    if "openai" in providers_in_use and not OPENAI_API_KEY:
        st.error("OpenAI API key is missing. Please set the OPENAI_API_KEY environment variable.")
        api_keys_ok = False

    if not api_keys_ok:
        st.warning("Processing cannot start due to missing API keys.")
        st.session_state.run_processing_flag = False # Reset flag
    else:
        total_files = len(uploaded_files)
        progress_bar_placeholder = st.empty() 
        status_text_placeholder = st.empty()   

        task_for_this_run = st.session_state.task_for_run
        llm_a_for_this_run = st.session_state.llm_a_for_run
        llm_b_for_this_run = st.session_state.llm_b_for_run

        with st.spinner(f"Processing {total_files} PDF(s) for '{task_for_this_run}' task... Please wait."):
            for i, uploaded_file in enumerate(uploaded_files):
                current_file_result = {
                    "filename": uploaded_file.name, 
                    "task_performed": task_for_this_run,
                    "llm_a_used": llm_a_for_this_run, # Store LLM used for this result
                    "llm_b_used": llm_b_for_this_run, # Store LLM used for this result
                    "step1_output": None, 
                    "step2_verification": None, 
                    "error_message": None,
                    "extracted_text_snippet": None, 
                    "time_step1_sec": None, 
                    "time_step2_sec": None  
                }
                
                progress_percentage = (i + 1) / total_files
                progress_bar_placeholder.progress(progress_percentage)
                status_text_placeholder.text(f"Processing file {i+1}/{total_files}: {uploaded_file.name}...")
                
                try:
                    status_text_placeholder.text(f"({i+1}/{total_files}) Extracting text from: {uploaded_file.name}")
                    source_text, extraction_error = extract_text_from_pdf(uploaded_file)
                    if extraction_error:
                        current_file_result["error_message"] = extraction_error
                        st.session_state.results.append(current_file_result)
                        continue 
                    current_file_result["extracted_text_snippet"] = (source_text[:500] + "...") if source_text and len(source_text) > 500 else source_text

                    if task_for_this_run == TASK_PARAGRAPH_SUMMARY:
                        prompt_step1_template = PROMPT_SUMMARY_GENERATION
                        prompt_step2_template = PROMPT_SUMMARY_VERIFICATION
                    elif task_for_this_run == TASK_EXPLAIN_MAIN_SUBJECT:
                        prompt_step1_template = PROMPT_EXPLAINER_TASK_FOR_LLM_A
                        prompt_step2_template = PROMPT_EXPLANATION_VERIFICATION_FOR_LLM_B
                    elif task_for_this_run == TASK_THIELIAN_LENS:
                        prompt_step1_template = PROMPT_THIELIAN_LENS_GENERATION_LLM_A
                        prompt_step2_template = PROMPT_THIELIAN_LENS_VERIFICATION_LLM_B
                    else:
                        current_file_result["error_message"] = f"Invalid task selected: {task_for_this_run}"
                        st.session_state.results.append(current_file_result)
                        continue
                    
                    status_text_placeholder.text(f"({i+1}/{total_files}) Performing Step 1 ({task_for_this_run}) for: {uploaded_file.name} using {llm_a_for_this_run}")
                    start_time_step1 = time.perf_counter()
                    step1_output, step1_error = get_llm_response(source_text, prompt_step1_template, llm_a_for_this_run)
                    end_time_step1 = time.perf_counter()
                    current_file_result["time_step1_sec"] = round(end_time_step1 - start_time_step1, 2)

                    if step1_error:
                        current_file_result["error_message"] = f"Step 1 ({task_for_this_run}) error: {step1_error}"
                        st.session_state.results.append(current_file_result)
                        continue
                    current_file_result["step1_output"] = step1_output

                    status_text_placeholder.text(f"({i+1}/{total_files}) Performing Step 2 (Verification) for: {uploaded_file.name} using {llm_b_for_this_run}")
                    start_time_step2 = time.perf_counter()
                    verification_output, verification_error = get_llm_response(source_text, prompt_step2_template, llm_b_for_this_run, step1_output_for_step2=step1_output)
                    end_time_step2 = time.perf_counter()
                    current_file_result["time_step2_sec"] = round(end_time_step2 - start_time_step2, 2)

                    if verification_error:
                        current_file_result["error_message"] = f"Step 2 (Verification) error: {verification_error}"
                        current_file_result["step2_verification"] = "Verification failed." 
                        st.session_state.results.append(current_file_result)
                        continue
                    current_file_result["step2_verification"] = verification_output
                
                except Exception as e:
                    current_file_result["error_message"] = f"Unexpected error processing {uploaded_file.name}: {str(e)}"
                
                st.session_state.results.append(current_file_result)

            status_text_placeholder.success(f"All files processed for '{task_for_this_run}' task!")
            progress_bar_placeholder.empty() 
        st.session_state.run_processing_flag = False # Reset the flag after processing is done or if API keys were missing

if st.session_state.results:
    st.markdown("---")
    st.header("üìä Processing Results")
    
    downloadable_content = "" 

    for i, res in enumerate(st.session_state.results): 
        task_label = res.get('task_performed', "N/A") # Get task from result item
        llm_a_used_for_res = res.get('llm_a_used', "N/A")
        llm_b_used_for_res = res.get('llm_b_used', "N/A")

        expander_title = f"Results for: {res['filename']} (Task: {task_label})"
        if res["error_message"]:
            expander_title += " (Error)"
        
        with st.expander(expander_title, expanded=False):
            downloadable_content += f"--- Results for: {res['filename']} ---\n"
            downloadable_content += f"Task Performed: {task_label}\n\n"
            
            if res["error_message"]:
                st.error(f"Error: {res['error_message']}")
                downloadable_content += f"Error: {res['error_message']}\n\n"
            
            step1_output_header_display = f"Step 1 Output (by {llm_a_used_for_res})" 
            if task_label == TASK_PARAGRAPH_SUMMARY:
                step1_output_header_display = f"Generated Paragraph Summary (by {llm_a_used_for_res})"
            elif task_label == TASK_EXPLAIN_MAIN_SUBJECT:
                step1_output_header_display = f"Generated Explanation (by {llm_a_used_for_res})"
            elif task_label == TASK_THIELIAN_LENS:
                 step1_output_header_display = f"Generated Thielian Lens Insight (by {llm_a_used_for_res})"


            if res["step1_output"]:
                st.subheader(step1_output_header_display)
                if res["time_step1_sec"] is not None:
                    st.caption(f"Step 1 Time: {res['time_step1_sec']:.2f} seconds")
                    downloadable_content += f"Step 1 Time: {res['time_step1_sec']:.2f} seconds\n"
                st.markdown(res["step1_output"]) 
                downloadable_content += f"{step1_output_header_display}:\n{res['step1_output']}\n\n"
            elif not res["error_message"]:
                st.info("Step 1 output could not be generated for this file.")
                downloadable_content += "Step 1 output could not be generated for this file.\n\n"

            if res["step2_verification"]:
                st.subheader(f"Step 2 Verification (by {llm_b_used_for_res}):")
                if res["time_step2_sec"] is not None:
                    st.caption(f"Step 2 Time: {res['time_step2_sec']:.2f} seconds")
                    downloadable_content += f"Step 2 Time: {res['time_step2_sec']:.2f} seconds\n"

                downloadable_content += f"Step 2 Verification (by {llm_b_used_for_res}):\n"
                if "GREEN LIGHT" in res["step2_verification"].upper(): 
                    st.success("‚úÖ GREEN LIGHT")
                    downloadable_content += "GREEN LIGHT\n"
                elif "RED LIGHT" in res["step2_verification"].upper(): 
                    st.error("‚ùå RED LIGHT")
                    downloadable_content += "RED LIGHT\n"
                    try:
                        reasons_part = res["step2_verification"].upper().split("RED LIGHT", 1)[1].strip()
                        if reasons_part:
                            st.markdown("**Reasons:**")
                            downloadable_content += "Reasons:\n"
                            reason_lines = reasons_part.split('\n')
                            for line in reason_lines:
                                clean_line = line.strip()
                                if clean_line.startswith("*") or clean_line.startswith("-"):
                                    st.markdown(clean_line)
                                    downloadable_content += f"{clean_line}\n"
                                elif clean_line: 
                                    st.markdown(f"* {clean_line}")
                                    downloadable_content += f"* {clean_line}\n"
                    except IndexError: 
                        pass 
                else: 
                    st.info(res["step2_verification"]) 
                    downloadable_content += f"{res['step2_verification']}\n"
                downloadable_content += "\n"

            elif not res["error_message"] and res["step1_output"]: 
                 st.info("Step 1 output was generated but Step 2 (Verification) did not complete or returned no output.")
                 downloadable_content += "Step 1 output was generated but Step 2 (Verification) did not complete or returned no output.\n\n"
            
            downloadable_content += "\n\n" 

    if downloadable_content: 
        st_copy_to_clipboard(downloadable_content, "Copy All Results to Clipboard")
        st.caption("Click the button above to copy all results to your clipboard.")


st.markdown("---")
st.caption("Developed with Streamlit. Ensure API keys (GROQ_API_KEY, OPENAI_API_KEY) are set as environment variables.")
st.caption(f"Using models (check API docs for exact IDs): {', '.join(MODEL_OPTIONS.keys())}")
st.caption("Ensure you have `streamlit-copy-to-clipboard` installed: `pip install streamlit-copy-to-clipboard`")

